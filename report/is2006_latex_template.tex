\documentclass{article}
\usepackage{interspeech2006,amssymb,amsmath,epsfig}
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{natbib} % Harvard style bib
\bibliographystyle{IEEEtranN}

\usepackage{lipsum} % Dummy text

\setcounter{page}{1}
\sloppy		% better line breaks
\ninept
%SM below a registered trademark definition
\def\reg{{\rm\ooalign{\hfil
     \raise.07ex\hbox{\scriptsize R}\hfil\crcr\mathhexbox20D}}}


\title{Dockerised Clouds: \\ A Comparative Study on Container Cluster Management Frameworks}

\makeatletter
\def\name#1{\gdef\@name{#1\\}}
\makeatother
\name{{\em Anthony Troy, Martin Somers and Marcelo Grossi}}

\address{School of Computing  \\
Dublin City University \\
Dublin 9, Ireland\\
{\small \tt\{anthony.troy3,martin.somers,marcelo.grossi2\}@mail.dcu.ie}
}


\begin{document}

\maketitle

\begin{abstract}
Containerisation is a recently resurged computing paradigm 
that is having a significant impact on how applications are being built, 
shipped and ran.\ Being interoperable in nature, container instances can be easily scaled
on a single host or across a cluster of hosts.\
Docker, albeit a relatively young project, 
has successfully established a container standard  
in Linux and poses itself as being production-ready.\ 
Increasingly, cluster management frameworks are providing 
first-class support for the Docker container standard and runtime. 
This includes established solutions such as Apache Mesos 
and Kubernetes.\ Docker now also includes its own native clustering
 tool, Swarm.
This paper considers Docker's production-readiness with respect
to native clustering capabilities and runtime interoperability.\
We review the current approaches and patterns forwarded by 
cluster management solutions to orchestrate distributed container clusters under Docker,\
and subsequently contribute a comparative analysis of two varying 
frameworks in this space,  Kubernetes and Swarm.


\end{abstract}

\section{Introduction}
Containers have a long history in computing though much of their recent popularity 
surround the recent developments of both LXC and the Docker platform. 
The former can be described as a container execution environment,
or more formally, a Linux user space interface to 
access new kernel capabilities of achieving process isolation through namespaces
and cgroups \citep{Claus}. The latter is an open-source suite of tools managed by Docker Inc.\ which
extends upon container technology such as LXC, in turn 
allowing containers to behave like ``full-blown hosts in their own right" 
whereby containers have ``strong isolation, their own network and storage stacks, as well 
as resource management capabilities to allow friendly co-existence of multiple containers on a host" \citep{db}.
\par 
Uncertainties around Docker's maturity and production-readiness have been expressed \citep{Kereki, Powers, Merkel}, however 
over the last two years the states of both Docker and the containerisation ecosystem continue to rapidly progress.\
Last year Docker has seen an unprecedented increase in development, adoption and community uptake \citep{Merkel}. Most
notably was the introduction of customisable container execution environments. This means as opposed to LXC one can
``take advantage of the numerous isolation tools available" such as ``OpenVZ, systemd-nspawn, libvirt-sandbox, qemu/kvm, BSD Jails and Solaris Zones".
Also included in this 0.9 release was the new built-in container execution driver ``libcontainer", which replaced LXC as the default driver.
Going forward on all platforms Docker can now execute kernel features such as ``namespaces, control groups, capabilities, apparmor profiles, 
network interfaces and firewalling rules" predictably ``without depending on LXC" as an external dependency \citep{Hykes}. 
\par
Interestingly, libcontainer itself was the first project to provide a standard interface for making containers and managing their lifecycle.\
Subsequently the Docker CEO  announced the coming together of industry leaders and others in partnership with the Linux Foundation
to form a ``minimalist, non-profit, openly governed project" named The Open Container Initiative (OCI), with the purpose of defining 
``common specifications around container format and runtime" \citep{Golub}. 
Thereafter Docker donated its base container format and runtime, libcontainer, to be maintained by the OCI. 
\par
Amidst establishing a container standard, Docker has made significant headway in 
supporting multi-host cloud production environments. In terms of native tooling, in the last year Docker has implemented
a suite of tools for provisioning and orchestrating containers:
\begin{itemize}
\item \textbf{Docker Machine} allows one to provision Docker hosts, which are simply Linux virtual machines (VMs) supporting Docker, on a local machine or cloud. 
Its pluggable driver API currently supports ``provisioning Docker locally with Virtualbox as well as remotely" on cloud providers such Digital Ocean, AWS, Azure and VMware.
\item \textbf{Docker Swarm} is a clustering solution which takes the standard 
``Docker Engine and extends it to enable you to work on a cluster of containers". 
This in turn allows one to ``manage a resource pool of Docker hosts and schedule
containers to run transparently on top, automatically managing workload and providing fail-over services", whereby you can specify how each container is to be ran in the the cluster, in turn allowing one to orchestrate and choreograph local or cloud containers.
\item \textbf{Docker Compose} is the ``glue" allowing one to configure relationships between containers (called links) and define within a single configuration file a full container based application and it's interdependencies, along with resource needs (like memory or disk).
\end{itemize}
\noindent In many cases an existing cloud infrastructure depends upon one or more orchestration tools, for example 
Consul for service discovery. Typically, such tools cannot be migrated away from easily and in turn cause ``vendor lock-in".
Consequently, Docker have implemented this trio of orchestration tools in a generic way, 
providing ``a standard interface to service providers so that they can almost be used as plug-and-play solutions" on top of the Docker platform \citep{holla}.
\par
Backed by both the industry and community, Docker is now presented as a more mature and production-ready
platform, however it should be mentioned that there yet exists no
formal topology for a fully, or partly, Dockerised cloud \citep{Claus}. In 
turn, there is no widely accepted solution for managing distributed 
Docker-based clusters.\ This is unsurprising given that 
practitioners and industry experts have noted that frameworks in this space
vary greatly in terms of capability, architecture and target cluster proportion
\citep{goasguen, holla}.
\par
This paper presents TODO. The remainder of this paper is structured as follows. TODO


\section{Study Design}
\lipsum[1]

\subsection{Docker Swarm}
A part of the docker's native ecosystem, swarm is docker's production-ready cluster management and orchestration tool. It allows for transparent scaling of docker containers across multiple hosts and because it serves the same docker daemon API it can be used seamlessly with any docker compatible tools (like Docker Compose, Dokky, Jenkins and others).
Swarm will run a swarm agent daemon in each node of the cluster and coordinate them using a pluggable discovery service back-end (which maintains a list of nodes in your cluster) and a swarm manager (that will actually control the whole cluster). The connection between the manager and agent nodes is made through a secure tcp connection (by means of TSL certificates). In previous versions of swarm the manager itself was the single point of failure of the cluster as there was no fail-over available. In the latest release however, version 1.0, a replication service for the manager has been developed whereby several manager instances will compete for leadership, making swarm a highly available solution.
\subsubsection{Networking}
Making use of the new networking feature of docker (as of version 1.9), the whole cluster is interconnected by a virtual network that can be manually configured to connect any nodes in private/global scopes. The overlay network driver is used by default which results in all nodes of a cluster to be on the same virtual network (and so able to intercommunicate out-of-the-box). This is a very powerful feature for multi-host configuration that is now supported natively by docker.
\subsubsection{Scheduling strategies}
When a new container is to be created, swarm uses one of three scheduling algorithms for determining in which node the image will be deployed to and subsequently ran.
\begin{itemize}
\item \textbf{Spread} as the name says will try to evenly spread the containers in as many nodes as possible, taking into account the available CPU and RAM and also the number of running containers. This is the default algorithm.
\item \textbf{Binpack} also considers the node's available resources but unlike spread, it will try to schedule as many containers on the same node as possible. This is the ideal algorithm for maximizing the number of running containers on the cluster, minimizing resource fragmentation.
\item \textbf{Random} simply assigns a new container to a node in random fashion, not taking into consideration any other measurements.
\end{itemize}
\subsubsection{Filtering}
To support more complex scheduling mechanism swarm also provides filtering to effectively subset the available nodes based on a given criteria.
\begin{itemize}
\item \textbf{Constraint}* uses key-value pairs that are set at node level and are used as labels to identify characteristics of the node. This filter can be used on container creation by the user to select only nodes with a particular feature (key-value pair). This approach has several practical use cases such as tagging nodes on their physical location (region=eu-ireland or region=us-west), the running environment (environment=production or environment=test), hardware characteristics (storage=ssd or gpu=nvidia) or any other logical partitioning.
\item \textbf{Affinity}* is used to create an "attraction" between containers (to run next to another container), images (to run on nodes that already have an image pulled) or a label (the label is created upon a containers creation).
\item \textbf{Port} considers exposed ports on the node as unique resources, and will try to schedule containers on nodes that have a particular port available.
\item \textbf{Dependency} will try to honour volume, network stack or link dependencies and schedule containers only on nodes that match.
\item \textbf{Health} will prevent scheduling of new containers on unhealthy nodes.
\end{itemize}
\textbf{*} Both constraint and affinity filters are hard enforced, which means that if the affinity or constraint is not met, the container will not be started. The user has an option to create the affinity or constraint in soft mode, which will make the scheduler disregard the filter and use the configured strategy in case the affinity or constraint is not met.
\subsubsection{Not yet implemented}
Swarm does not yet support (as of version 1.0) host rebalancing - where a host fails and needs to be rescheduled on another node. This is in the roadmap and should be released alongside Docker version 1.10 release [https://github.com/docker/swarm/wiki/0.6.0-Milestone-Project-Page].


\subsection{Kubernetes}
\lipsum[1]


\section{Discussion}
\lipsum[1]

\section{Conclusions}
\lipsum[1]


\vspace{-7.5mm}
\renewcommand{\refname}{\section{References}}
\bibliography{is2006_latex_template}

\end{document}
