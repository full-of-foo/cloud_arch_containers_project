\documentclass{article}
\usepackage{interspeech2006,amssymb,amsmath,epsfig}
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{natbib} % Harvard style bib
\bibliographystyle{IEEEtranN}

\usepackage{lipsum} % Dummy text

\setcounter{page}{1}
\sloppy		% better line breaks
\ninept
%SM below a registered trademark definition
\def\reg{{\rm\ooalign{\hfil
     \raise.07ex\hbox{\scriptsize R}\hfil\crcr\mathhexbox20D}}}


\title{Dockerised Clouds: \\ A Comparative Study on Container Cluster Management Frameworks}

\makeatletter
\def\name#1{\gdef\@name{#1\\}}
\makeatother
\name{{\em Anthony Troy, Martin Somers and Marcelo Grossi}}

\address{School of Computing  \\
Dublin City University \\
Dublin 9, Ireland\\
{\small \tt\{anthony.troy3,martin.somers,marcelo.grossi2\}@mail.dcu.ie}
}


\begin{document}

\maketitle

\begin{abstract}
Containerisation is a recently resurged computing paradigm 
that is having a significant impact on how applications are being built, 
shipped and ran.\ Being interoperable in nature, container instances can be easily scaled
on a single host or across a cluster of hosts.\
Docker, albeit a relatively young project, 
has successfully established a container standard  
in Linux and poses itself as being production-ready.\ 
Increasingly, cluster management frameworks are providing 
first-class support for the Docker container standard and runtime. 
This includes established solutions such as Apache Mesos 
and Kubernetes.\ Docker now also includes its own native clustering
 tool, Swarm.
This paper considers Docker's production-readiness with respect
to native clustering capabilities and runtime interoperability.\
We review the current approaches and patterns forwarded by 
cluster management solutions to orchestrate distributed container clusters under Docker,\
and subsequently contribute a comparative analysis of two varying 
frameworks in this space, Kubernetes and Swarm.


\end{abstract}

\section{Introduction}
Containers have a long history in computing though much of their recent popularity 
surround the recent developments of both LXC and the Docker platform. 
The former can be described as a container execution environment,
or more formally, a Linux user space interface to 
access new kernel capabilities of achieving process isolation through namespaces
and cgroups \citep{Claus}. The latter is an open-source suite of tools managed by Docker Inc.\ which
extends upon container technology such as LXC, in turn 
allowing containers to behave like ``full-blown hosts in their own right" 
whereby containers have ``strong isolation, their own network and storage stacks, as well 
as resource management capabilities to allow friendly co-existence of multiple containers on a host" \citep{db}.
\par 
Uncertainties around Docker's maturity and production-readiness have been expressed \citep{Kereki, Powers, Merkel}, however 
over the last two years the states of both Docker and the containerisation ecosystem continue to rapidly progress.\
Last year Docker has seen an unprecedented increase in development, adoption and community uptake \citep{Merkel}. Most
notably was the introduction of customisable container execution environments. This means as opposed to LXC one can
``take advantage of the numerous isolation tools available" such as ``OpenVZ, systemd-nspawn, libvirt-sandbox, qemu/kvm, BSD Jails and Solaris Zones".
Also included in this 0.9 release was the new built-in container execution driver ``libcontainer", which replaced LXC as the default driver.
Going forward on all platforms Docker can now execute kernel features such as ``namespaces, control groups, capabilities, apparmor profiles, 
network interfaces and firewalling rules" predictably ``without depending on LXC" as an external dependency \citep{Hykes}. 
\par
Interestingly, libcontainer itself was the first project to provide a standard interface for making containers and managing their lifecycle.\
Subsequently the Docker CEO  announced the coming together of industry leaders and others in partnership with the Linux Foundation
to form a ``minimalist, non-profit, openly governed project" named The Open Container Initiative (OCI), with the purpose of defining 
``common specifications around container format and runtime" \citep{Golub}. 
Thereafter Docker donated its base container format and runtime, libcontainer, to be maintained by the OCI. 
\par
Amidst establishing a container standard, Docker has made significant headway in 
supporting multi-host cloud production environments. In terms of native tooling, in the last year Docker has implemented
a suite of tools for provisioning and orchestrating containers:
\begin{itemize}
\item \textbf{Docker Machine} allows one to provision Docker hosts, which are simply Linux virtual machines (VMs) supporting Docker, on a local machine or cloud. 
Its pluggable driver API currently supports ``provisioning Docker locally with Virtualbox as well as remotely" on cloud providers such Digital Ocean, AWS, Azure and VMware.
\item \textbf{Docker Swarm} is a clustering solution which takes the standard 
``Docker Engine and extends it to enable you to work on a cluster of containers". 
This in turn allows one to ``manage a resource pool of Docker hosts and schedule
containers to run transparently on top, automatically managing workload and providing fail-over services", whereby you can specify how each container is to be ran in the cluster, in turn allowing one to orchestrate and choreograph local or cloud containers.
\item \textbf{Docker Compose} is the ``glue" allowing one to configure relationships between containers (called links) and define within a single configuration file a full container based application and it's interdependencies, along with resource needs (like memory or disk).
\end{itemize}
\noindent In many cases an existing cloud infrastructure depends upon one or more orchestration tools, for example 
Consul for service discovery. Typically, such tools cannot be migrated away from easily and in turn cause ``vendor lock-in".
Consequently, Docker have implemented this trio of orchestration tools in a generic way, 
providing ``a standard interface to service providers so that they can almost be used as plug-and-play solutions" on top of the Docker platform \citep{holla}.
\par
Backed by both the industry and community, Docker is now presented as a more mature and production-ready
platform. Notwithstanding this impressive growth, there yet exists no
formal topology for a fully, or partly, Dockerised cloud \citep{Claus}.\ Similarly,
so for there is no widely accepted solution for managing 
Docker container clusters.\
 This paper considers how primary cluster management activities are achievable on top of the Docker platform.
In doing so we review the interoperability of the Docker runtime and format with existing 
container management solutions. Subsequently, we forward a comparative study
on two contrasting solutions, Kubernetes and Docker Swarm. 
\par
The remainder of this paper is structured as follows. In
section two we overview service orchestration, discovery and configuration in Docker clusters. Following this, section three and four detail our
reviews of Docker Swarm and Kubernetes respectively. Finally, in section five we conclude this paper.


\section{Docker Cluster Management}
Practitioners and industry experts note that cluster management tooling supporting Docker
vary greatly in terms of capability, architecture and target cluster proportion
\citep{goasguen, holla}. This is unsurprising when we consider that all infrastructures 
are not subject to same orchestration requirements and software release cycles.
For instance, slow moving infrastructures can be characterised as having infrequent application deployments,
 hard-coded service configurations and rare service failures which may not have an urgent impact. In contrast, more fast moving infrastructures feature continuous deployments and strong automation in terms of service configuration and recovery. 
 % maybe add more of an intro here

\subsection{Service orchestration}
Central to cloud cluster management is the ability to elastically provision and tear down clusters. Many cloud providers have introduced their own service orchestration tools such as CloudFormation from AWS and Heat by OpenStack \citep{Dudouet}. 
On a high-level, these tools simply define a cluster template which can be later orchestrated with possibly extended configurations. As previously mentioned, the native Docker orchestration tools support similar features that can clusterise multi-host containers. Docker Compose conceptually defines a similar template to that of Amazon's CloudFormation and allows one to perform orchestration tasks such as provisioning, destroying and scaling on per container basis.
\par
\citet{Claus} describe container-based clusters as consisting of several hosts which are ``virtual servers on hypervisors or possibly bare-metal servers", each of which typically runs several containers that are responsible for scheduling, load balancing and serving an application or service. Meaning containers can be distributed across one or more host machines wherein these hosts might be virtual servers running other services that must also be orchestrated.
\par
Slow moving infrastructures may not be availing of their provider's orchestration tools as doing so is simply not required. Clusters themselves are manually defined once and the scaling of nodes can be introduced during deployments or at scheduled downtime. Nevertheless, Docker Compose supports this manual workflow. Conversely, fast moving infrastructures profit from their provider's orchestration tools, leveraging them to automate tasks around cluster management.\ As discussed previously, Swarm is a native Docker clustering tool for containers which pools Docker engines together into a single virtual host. In conjunction with Docker Compose, it facilitates for transparent orchestration across container clusters. \citep{holla}.
\par
Cluster management frameworks aim to abstract and automate service orchestration activities such as provisioning, scaling, task scheduling, resource utilisation management and failover recovery. Some cloud providers have implemented such frameworks which sit on top of Swarm. For example, Amazon's EC2 Container Service (ECS) is one that uses a shared-state scheduling model to execute tasks on containerised EC2 instances via containers. Each host instance has a preinstalled ECS agent which allows clusterised containers communicate together and with the ECS console. Consequently, via scheduled tasks, ECS clusters can be transparently and dynamically orchestrated.
\par
Stand-alone Swarm or ECS may be fitting orchestration solutions for fast moving infrastructures, however larger-scale clouds that host hundreds or thousands of containers require high-level cluster management platforms such as Apache Mesos and Kubernetes. The former abstracts ``distributed hardware resources into
a single pool of resources" and can provide similar cluster management facilities to ECS when integrated with scheduling and service management tools such as Marathon. The later is a higher-level platform specifically designed for managing containerised applications across multiple hosts including mechanisms for service deployment, scaling and maintenance.

\subsection{Service discovery and configuration}

Service discovery and configuration management are central cluster management concepts in distributed systems and microservices-based architectures. Both of which are argued to overlap in nature. Service discovery can be described as an approach to achieve ``dynamic and automatic software system composition, configuration and adaptation" \citep{Yang}. Generally, service discovery implementations accomplish this by allowing application components/services discover information or configurations about their current and  neighbouring environments through a distributed key-value store.
\par
Whether operating under a fast or slow moving infrastructure, requiring a service discovery solution is generally related to having a service-orientated  architecture style. The more distributed a system becomes, the more regularly do services require information about their own and neighbouring environments. The tooling around service discovery ranges in terms of complexity and provided features. DNS (Domain Name Systems) is a well-known and commonly understood standard which allows us ``associate a name with the IP address of one or machines" where the name becomes an ``entry point to the IP address of the host running that service" \citep{Newman}. More advanced tools like Consul and Apache Zookeeper support both configuration management and service discovery. The former is designed specifically for service discovery and can use service health checking features to route traffic away from unhealthy nodes. The later is used for wider variety of cases such ``configuration management, synchronizing data between services, leader election, message queues and as a naming service" \citep{Newman}.
\par 
Container-based service discovery involves the ability to dynamically register and discover multi-host containers among their peers. \citet{holla} poses two techniques to accomplish discovery in Docker; integrating Swarm backend discovery tools or using default Docker features like names and links. Docker Swarm implements a hosted discovery service which uses generated tokens to discover cluster nodes. Being primarily concerned with orchestration, Swarm does not currently support dynamic service registration and configuration. However, to dynamically configure and manage the services in your containers one can use a discovery backend with Swarm such as Etcd, Consul or Zookeeper. 
\par
As previously highlighted, Docker Compose provides a mechanism to link named containers on the same host. This is accomplished by ``inserting the first container's IP address in /etc/hosts when starting the second container". Importantly, the IP address of a container living on a different host ``is not known by the docker daemon running in the current host". The ambassador container pattern achieves cross-host container linking between provider and consumer containers by dynamically configuring network connections through respective intermediate ambassador containers \citep{holla}.

\section{Study: Docker Swarm}
Now included as part of the Docker platform, Swarm is Docker's production-ready cluster management tool.  Quite simply, Swarm conceptually assembles a pool of Docker hosts into one virtual host \citep{swarm_manual}. It allows for the transparent scaling of containers across multiple hosts and, as it serves the Docker daemon API, it can be used seamlessly on top of any Docker supported tools (like Docker Compose, Dokky, Jenkins and others).
\par
Swarm currently adopts a single master-slave model. Given any amount of slave nodes, one predefined Swarm manager serves as the entry point for cluster administration. It is important to note that this manager node is indeed replicable for fail-overs\.
An agent daemon running on each node allows the nodes to communicate with the Swarm manager over the network.\ This connection is made through a secure TCP connection. 
\par
In terms of supporting cross-node communication, the native discovery service provides only features for node registration, where host IPs are simply whitelisted onto the cluster. Nevertheless, as previously highlighted, this discovery interface is substitutable for more advanced discovery tools \citep{holla}. 

\subsection{Networking and Persistent Storage}
One of the most appealing features of Swarm is it's inherent compatibility with the Docker Engine API. This allows one to transparently issue any Docker command across an entire cluster. Following the recent release of Docker 1.9, two of biggest weaknesses of Docker have been overcome: volume management and cross-container communication.
\par
A Swarm cluster is interconnected through a virtual network built upon basic Linux bridging features and virtual extensible LAN (or VXLANs) that can be manually configured to connect any nodes in private or global scopes. Living up to the "batteries included, but swappable" model, the native networking can be replaced by third-place products via the LibNetwork open source project \citep{Butler}.
\par
New containers are created by default in a bridge network (where bridge is the name of a built-in driver includes the container in the node's bridge). To achieve cross-container communication Docker provides an overlay network driver which ensures that all cluster nodes are on the same virtual network. It was the addition of these two features to the Docker Engine and Swarm which have deemed Docker as being "production ready" \citep{milestone}.


\subsection{Scheduling Strategies}
Upon creating container instances, Swarm can use one of three scheduling algorithms to determine which node the container will be deployed to and ran on.
\begin{itemize}
\item \textbf{Spread} attempts to evenly spread the containers in as many nodes as possible, taking into account the available CPU and RAM and also the number of running containers. This is the default algorithm.
\item \textbf{Binpack} also considers the node's available resources but, unlike spread, will try to schedule as many containers on the same node as possible. This is the ideal algorithm for maximizing the number of running containers on the cluster, minimizing resource fragmentation.
\item \textbf{Random} simply assigns a new container to a node in random fashion, not taking into consideration any other measurements.
\end{itemize}
\subsection{Filtered Scheduling}
To support more complex scheduling mechanism Swarm also provides filtering to effectively subset the available nodes based on a given criteria.
\begin{itemize}
\item \textbf{Constraint}\footnotemark[1] uses key-value pairs that are set at node level and are used as labels to identify characteristics of the node. This filter can be used on container creation by the user to select only nodes with a particular feature (key-value pair). This approach has several practical use cases such as tagging nodes on their physical location (region=eu-ireland or region=us-west), the running environment (environment=production or environment=test), hardware characteristics (storage=ssd or gpu=nvidia) or any other logical partitioning.
\item \textbf{Affinity}\footnotemark[1] is used to create an "attraction" between containers (to run next to another container), images (to run on nodes that already have an image pulled) or a label (the label is created upon a containers creation).
\item \textbf{Port} considers exposed ports on the node as unique resources, and will try to schedule containers on nodes that have a particular port available.
\item \textbf{Dependency} will try to honour volume, network stack or link dependencies and schedule containers only on nodes that match.
\item \textbf{Health} will prevent scheduling of new containers on unhealthy nodes.
\end{itemize}
\footnotetext[1]{Both constraint and affinity filters are hard enforced, which means that if the affinity or constraint is not met, the container will not be started. The user has an option to create the affinity or constraint in soft mode, which will make the scheduler disregard the filter and use the configured strategy in case the affinity or constraint is not met.}
\subsection{Not yet implemented}
As of version 1.0, Swarm does not yet support host rebalancing - where a host fails and needs to be rescheduled on another node. Nevertheless, this is noted on the project roadmap and is destined to be released alongside Docker version 1.10 \citep{Vieux}.

\section{Study: Kubernetes}
Like Docker, Kubernetes is an open source solution for orchestration management and a monitoring tool for large computation clusters. Kubernetes is built by the same team of engineers who developed the Google compute platform and extends on the experience those engineers have from managing and tooling internal clusters. Kubernetes is a scheduler similar to Mesos\footnote{http://mesos.apache.org/}, a resource scheduler which is used for monitoring and automated deployment.
\par
In order for a cluster to operate optimally individually processing units need to be able to communicate efficiently with each other. This is a rather difficult problem to solve for very large clusters. Kubernetes is a system for creating, running and most importantly managing distributed infrastructure. The framework allows for a granular approach to configuring a high availability cluster. At the heart of Kubernetes ethos are etcd and its internal scheduler which binds unscheduled pods to nodes via a binding API.
\par
Everything in Kubernetes is declarative within its own inner API definition. This definition is maintained by calling from kube-apiserver service. A Kubernetes cluster consists of a master and a node enity. Master components are responsible for making global decisions about a cluster including scheduling, detecting and responding to cluster events. Multiple nodes or minions may be assigned to a master. Etcd runs on each master/minion in a cluster and gracefully handles master election during network partitions and the loss of the current master. Natively etcd can recover from hardware failures and this is one of the key features that is driving Kubernetes market penetration. Other platforms are harnessing the power of open source etcd, including Cloudfoundry\footnote{https://www.cloudfoundry.org/}, Fleet\footnote{https://www.fleetio.com/} and Deis\footnote{http://deis.io/} infrastructure. Etcd forms the conical hub for the creation and management of distributed tasks via services and replication controllers. Pods are a native entity within the Kubernetes hierarchy ecosystem, similar to containers. Containers and pods can be considered one of the same where pods inherent a refined manifest to the management of it life cycle within a live cluster.  Lets consider the terminology used within a Kubernetes environment.
\subsection{Pods}
Pods form the basic build entity within a Kubernetes cluster. A pod may contain a heterogeneous mix of containers that collectively communicate together to maintain an application or the structure of the pod may also be singular in nature but all pods mimic a shared network interface. This results in a uniform interface across all pods connected to the cluster. Containers within the same pod communicate on a local host protocol and communication via external pods is carried out though a proxy interface via network services. This allows for highly effective access to pods and different containers in the same pod and prevents pods from using the same network ports. Pods once initialize will never be moved. If a Pod is lost or removed a new pod is created via the replication controller to follow the manifest of optimum resources laid down in that file.
\subsection{Replication Controllers}
While monitoring the integrity of a live cluster replication controllers may be called upon to manage the infrastructure of the application depending on the manifest. Replication Controllers serve as a template manifest to the dynamic allocate resources that are available to the cluster and the initial creation and monitoring of these resources is maintained via replication controller yaml file.  Using this file you can specify the number of replicas that you want a pod to have. If there are too many pods the status quo will be restored to the amount of pods defined using the –replica tag. The same will hold if there are too few and more pods shall be created.  The replication controller maintains the desired number of pods and makes sure its label selectors are operational.
\subsection{Labels}
Labels are in effect a naming convention that are assigned to pods for management of the computation resources. This naming schema may be used to roll out a testing/development application or a full production implementation depending on the use case. Labeling gives clients the ability to easily access the current resources that are available scaling up or down as the need requires.
Unlike a naming convection using ID’s, labels do not provide uniqueness, many objects can contain the same label but kubernetes labels define a taxonomy of resources that are available.
\subsection{Network Services}
Network services provide the communication of not just pods but the communication between clusters. Communication within a Kubernetes cluster happens through a rigorous API that is accessed via the kubectl tool and kube-proxy. On node initialize kubectl is placed on every node while kube-proxy facilitates a kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding. Kubelet manages pods their children, their images and their volumes.

\section{Conclusions}
Being the native cluster management tool for the Docker platform, Swarm is inherently coupled with the Docker runtime interface. This can greatly simplify the process of clusterising existing container workflows. Nevertheless, given that Swarm is lightweight in nature, it does not support complex or custom scheduling akin to other solutions in this space. Furthermore, its native service discovery and replication features are arguably somewhat naive. In general, a requirement of reliable and fault-tolerant web application clusters is the ability to replicate service instances seamlessly, that is, without any downtime.
\par
Conversely, Kubernetes is feature-rich but somewhat lacks simplicity. It includes baked-in extensible features around discovery, replication, monitoring, scheduling and self-healing. However, one can argue that Kubernetes imposes quite an opinionated cluster management topology, whereby cluster constructs are grouped into logical collections of hosts, service tiers and containers. In contrast to orchestration in Swarm, Kuberentes offers a much higher-level abstraction over container deployments and supports VM provisioning. Although Kubernetes may require a more difficult migration path than Swarm, when used correctly it will likely yield a more fault-tolerant and scalable system.

\vspace{-7.5mm}
\renewcommand{\refname}{\section{References}}
\bibliography{is2006_latex_template}

\end{document}
